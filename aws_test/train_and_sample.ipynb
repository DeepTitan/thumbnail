{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-_UVMZCIAq_r"
   },
   "outputs": [],
   "source": [
    "# # Install dependencies\n",
    "\n",
    "# !git clone https://github.com/openai/CLIP\n",
    "# !git clone https://github.com/crowsonkb/guided-diffusion\n",
    "# !pip install -e ./CLIP\n",
    "# !pip install -e ./guided-diffusion\n",
    "# !pip install lpips\n",
    "# !pip install blobfile\n",
    "# !mkdir ckpt_model\n",
    "# ##RESTART KERNEL###\n",
    "# Imports\n",
    "\n",
    "import gc\n",
    "import io\n",
    "import math\n",
    "import sys\n",
    "\n",
    "from IPython import display\n",
    "import lpips\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "\n",
    "sys.path.append('./CLIP')\n",
    "sys.path.append('./guided-diffusion')\n",
    "sys.path.append('./improved-diffusion')\n",
    "\n",
    "import clip\n",
    "from guided_diffusion.script_util import create_model_and_diffusion, model_and_diffusion_defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open test_summaries.json\n",
    "import json\n",
    "with open('test_summaries.json') as json_file:\n",
    "    test_summaries = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/isaiahwilliams/thumbnail/aws_test/train_and_sample.ipynb Cell 3'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/isaiahwilliams/thumbnail/aws_test/train_and_sample.ipynb#ch0000021?line=55'>56</a>\u001b[0m     vals \u001b[39m=\u001b[39m vals \u001b[39m+\u001b[39m [\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m1\u001b[39m\u001b[39m'\u001b[39m][\u001b[39mlen\u001b[39m(vals):]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/isaiahwilliams/thumbnail/aws_test/train_and_sample.ipynb#ch0000021?line=56'>57</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m vals[\u001b[39m0\u001b[39m], \u001b[39mfloat\u001b[39m(vals[\u001b[39m1\u001b[39m])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/isaiahwilliams/thumbnail/aws_test/train_and_sample.ipynb#ch0000021?line=59'>60</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mMakeCutouts\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/isaiahwilliams/thumbnail/aws_test/train_and_sample.ipynb#ch0000021?line=60'>61</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, cut_size, cutn, cut_pow\u001b[39m=\u001b[39m\u001b[39m1.\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/isaiahwilliams/thumbnail/aws_test/train_and_sample.ipynb#ch0000021?line=61'>62</a>\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import re\n",
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return sentences\n",
    "\n",
    "\n",
    "# Define necessary functions\n",
    "\n",
    "def fetch(url_or_path):\n",
    "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
    "        r = requests.get(url_or_path)\n",
    "        r.raise_for_status()\n",
    "        fd = io.BytesIO()\n",
    "        fd.write(r.content)\n",
    "        fd.seek(0)\n",
    "        return fd\n",
    "    return open(url_or_path, 'rb')\n",
    "\n",
    "\n",
    "def parse_prompt(prompt):\n",
    "    if prompt.startswith('http://') or prompt.startswith('https://'):\n",
    "        vals = prompt.rsplit(':', 2)\n",
    "        vals = [vals[0] + ':' + vals[1], *vals[2:]]\n",
    "    else:\n",
    "        vals = prompt.rsplit(':', 1)\n",
    "    vals = vals + ['', '1'][len(vals):]\n",
    "    return vals[0], float(vals[1])\n",
    "\n",
    "\n",
    "class MakeCutouts(nn.Module):\n",
    "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
    "        super().__init__()\n",
    "        self.cut_size = cut_size\n",
    "        self.cutn = cutn\n",
    "        self.cut_pow = cut_pow\n",
    "\n",
    "    def forward(self, input):\n",
    "        sideY, sideX = input.shape[2:4]\n",
    "        max_size = min(sideX, sideY)\n",
    "        min_size = min(sideX, sideY, self.cut_size)\n",
    "        cutouts = []\n",
    "        for _ in range(self.cutn):\n",
    "            size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
    "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
    "            offsety = torch.randint(0, sideY - size + 1, ())\n",
    "            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
    "            cutouts.append(F.adaptive_avg_pool2d(cutout, self.cut_size))\n",
    "        return torch.cat(cutouts)\n",
    "\n",
    "\n",
    "def spherical_dist_loss(x, y):\n",
    "    x = F.normalize(x, dim=-1)\n",
    "    y = F.normalize(y, dim=-1)\n",
    "    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)\n",
    "\n",
    "\n",
    "def tv_loss(input):\n",
    "    \"\"\"L2 total variation loss, as in Mahendran et al.\"\"\"\n",
    "    input = F.pad(input, (0, 1, 0, 1), 'replicate')\n",
    "    x_diff = input[..., :-1, 1:] - input[..., :-1, :-1]\n",
    "    y_diff = input[..., 1:, :-1] - input[..., :-1, :-1]\n",
    "    return (x_diff**2 + y_diff**2).mean([1, 2, 3])\n",
    "\n",
    "\n",
    "def range_loss(input):\n",
    "    return (input - input.clamp(-1, 1)).pow(2).mean([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_image(save_path, prompt, model_dir):\n",
    "    # Model settings\n",
    "\n",
    "    model_config = model_and_diffusion_defaults()\n",
    "    model_config.update({\n",
    "        #'attention_resolutions': '32, 16, 8',\n",
    "        'class_cond': False,\n",
    "        'diffusion_steps': 4000,\n",
    "        'rescale_timesteps': True,\n",
    "        'timestep_respacing': '250',  # Modify this value to decrease the number of\n",
    "                                    # timesteps.\n",
    "        'image_size': 64,\n",
    "        'learn_sigma': True,\n",
    "        'noise_schedule': 'linear',\n",
    "        'num_channels': 128,\n",
    "        #'num_head_channels': 64,\n",
    "        'num_res_blocks': 3,\n",
    "        #'resblock_updown': True,\n",
    "        'use_checkpoint': False,\n",
    "        'use_fp16': True,\n",
    "        'use_scale_shift_norm': True,\n",
    "    })\n",
    "\n",
    "\n",
    "    # Load models\n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print('Using device:', device)\n",
    "\n",
    "    model, diffusion = create_model_and_diffusion(**model_config)\n",
    "    model.load_state_dict(torch.load(model_dir, map_location='cpu'))\n",
    "    model.requires_grad_(False).eval().to(device)\n",
    "    if model_config['use_fp16']:\n",
    "        model.convert_to_fp16()\n",
    "\n",
    "    clip_model = clip.load('ViT-B/16', jit=False)[0].eval().requires_grad_(False).to(device)\n",
    "    clip_size = clip_model.visual.input_resolution\n",
    "    normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                                    std=[0.26862954, 0.26130258, 0.27577711])\n",
    "    lpips_model = lpips.LPIPS(net='vgg').to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    prompts = [prompt]\n",
    "    image_prompts = []\n",
    "    batch_size = 16\n",
    "    clip_guidance_scale = 1000  # Controls how much the image should look like the prompt.\n",
    "    tv_scale = 150              # Controls the smoothness of the final output.\n",
    "    range_scale = 50            # Controls how far out of range RGB values are allowed to be.\n",
    "    cutn = 4\n",
    "    n_batches = 1\n",
    "    init_image = None   # This can be an URL or Colab local path and must be in quotes.\n",
    "    skip_timesteps = 0  # This needs to be between approx. 200 and 500 when using an init image.\n",
    "                        # Higher values make the output look more like the init.\n",
    "    init_scale = 0      # This enhances the effect of the init image, a good value is 1000.\n",
    "    seed = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def do_run():\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "\n",
    "        make_cutouts = MakeCutouts(clip_size, cutn)\n",
    "        side_x = side_y = model_config['image_size']\n",
    "\n",
    "        target_embeds, weights = [], []\n",
    "\n",
    "        for prompt in prompts:\n",
    "            txt, weight = parse_prompt(prompt)\n",
    "            target_embeds.append(clip_model.encode_text(clip.tokenize(txt).to(device)).float())\n",
    "            weights.append(weight)\n",
    "\n",
    "        for prompt in image_prompts:\n",
    "            path, weight = parse_prompt(prompt)\n",
    "            img = Image.open(fetch(path)).convert('RGB')\n",
    "            img = TF.resize(img, min(side_x, side_y, *img.size), transforms.InterpolationMode.LANCZOS)\n",
    "            batch = make_cutouts(TF.to_tensor(img).unsqueeze(0).to(device))\n",
    "            embed = clip_model.encode_image(normalize(batch)).float()\n",
    "            target_embeds.append(embed)\n",
    "            weights.extend([weight / cutn] * cutn)\n",
    "\n",
    "        target_embeds = torch.cat(target_embeds)\n",
    "        weights = torch.tensor(weights, device=device)\n",
    "        if weights.sum().abs() < 1e-3:\n",
    "            raise RuntimeError('The weights must not sum to 0.')\n",
    "        weights /= weights.sum().abs()\n",
    "\n",
    "        init = None\n",
    "        if init_image is not None:\n",
    "            init = Image.open(fetch(init_image)).convert('RGB')\n",
    "            init = init.resize((side_x, side_y), Image.LANCZOS)\n",
    "            init = TF.to_tensor(init).to(device).unsqueeze(0).mul(2).sub(1)\n",
    "\n",
    "        cur_t = None\n",
    "\n",
    "        def cond_fn(x, t, out, y=None):\n",
    "            n = x.shape[0]\n",
    "            fac = diffusion.sqrt_one_minus_alphas_cumprod[cur_t]\n",
    "            x_in = out['pred_xstart'] * fac + x * (1 - fac)\n",
    "            clip_in = normalize(make_cutouts(x_in.add(1).div(2)))\n",
    "            image_embeds = clip_model.encode_image(clip_in).float()\n",
    "            dists = spherical_dist_loss(image_embeds.unsqueeze(1), target_embeds.unsqueeze(0))\n",
    "            dists = dists.view([cutn, n, -1])\n",
    "            losses = dists.mul(weights).sum(2).mean(0)\n",
    "            tv_losses = tv_loss(x_in)\n",
    "            range_losses = range_loss(out['pred_xstart'])\n",
    "            loss = losses.sum() * clip_guidance_scale + tv_losses.sum() * tv_scale + range_losses.sum() * range_scale\n",
    "            if init is not None and init_scale:\n",
    "                init_losses = lpips_model(x_in, init)\n",
    "                loss = loss + init_losses.sum() * init_scale\n",
    "            return -torch.autograd.grad(loss, x)[0]\n",
    "\n",
    "        if model_config['timestep_respacing'].startswith('ddim'):\n",
    "            sample_fn = diffusion.ddim_sample_loop_progressive\n",
    "        else:\n",
    "            sample_fn = diffusion.p_sample_loop_progressive\n",
    "\n",
    "        for i in range(n_batches):\n",
    "            cur_t = diffusion.num_timesteps - skip_timesteps - 1\n",
    "\n",
    "            samples = sample_fn(\n",
    "                model,\n",
    "                (batch_size, 3, side_y, side_x),\n",
    "                clip_denoised=False,\n",
    "                model_kwargs={},\n",
    "                cond_fn=cond_fn,\n",
    "                progress=True,\n",
    "                skip_timesteps=skip_timesteps,\n",
    "                init_image=init,\n",
    "                randomize_class=True,\n",
    "                cond_fn_with_grad=True,\n",
    "            )\n",
    "\n",
    "            for j, sample in enumerate(samples):\n",
    "                cur_t -= 1\n",
    "                if j % 100 == 0 or cur_t == -1:\n",
    "                    print()\n",
    "                    for k, image in enumerate(sample['pred_xstart']):\n",
    "                        filename = f'progress_{i * batch_size + k:05}.png'\n",
    "                        TF.to_pil_image(image.add(1).div(2).clamp(0, 1)).save(save_path + '_' + str(i) + '_' + str(j) + '_' + str(k) + '.png')\n",
    "                        tqdm.write(f'Batch {i}, step {j}, output {k}:')\n",
    "                        display.display(display.Image(filename))\n",
    "                #save \n",
    "                        \n",
    "\n",
    "    gc.collect()\n",
    "    do_run()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all keys in test_summaries\n",
    "#loop through test_summaries and print values\n",
    "\n",
    "for key in test_summaries:\n",
    "    caption = test_summaries[key]['caption']\n",
    "    print('caption: ',caption)\n",
    "    finetuned= 'final_model/final/final.pt'\n",
    "    base = './imagenet64_uncond_100M_1500K.pt'\n",
    "    gen_image(caption, finetuned)\n",
    "    gen_image(caption, base)\n",
    "    summary = test_summaries[key]['summary']\n",
    "    #split into sentences\n",
    "    sentences = split_into_sentences(summary)\n",
    "    if sentences == []:\n",
    "        sentences = summary\n",
    "    else:\n",
    "        sentences = sentences[0]\n",
    "    print('summ: ', sentences)\n",
    "    gen_image(sentences, finetuned)\n",
    "    gen_image(sentences, base)\n",
    "\n",
    "\n",
    "    print('\\n')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DOWNLOAD PRETRAINED WEIGHTS\n",
    "#!curl -OL \"https://openaipublic.blob.core.windows.net/diffusion/march-2021/imagenet64_uncond_100M_1500K.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JmbrcrhpBPC6"
   },
   "outputs": [],
   "source": [
    "#get all keys in test_summaries\n",
    "#loop through test_summaries and print values\n",
    "\n",
    "for key in test_summaries:\n",
    "    \n",
    "    caption = test_summaries[key]['caption']\n",
    "\n",
    "    #reduce caption to 50 words\n",
    "    caption = caption.split(' ')\n",
    "    caption = caption[:50]\n",
    "    caption = ' '.join(caption)\n",
    "    print('caption: ',caption)\n",
    "    finetuned= 'final_model/final/final.pt'\n",
    "    base = './imagenet64_uncond_100M_1500K.pt'\n",
    "    save_path = f'./fake/caption/base/cb{key}'\n",
    "\n",
    "    #check if any path beginning with save_path exists\n",
    "    if os.path.exists(save_path + \n",
    "\n",
    "    #get current directory\n",
    "    curr_dir = os.getcwd()\n",
    "    save_path = os.path.join(curr_dir, save_path[1:])\n",
    "    if not os.path.exists(save_path):\n",
    "        print('already exists')\n",
    "        continue\n",
    "    gen_image(save_path, caption, base, 'ddim500')\n",
    "    save_path = f'./fake/caption/ft/cf{key}'\n",
    "    gen_image(save_path, caption, finetuned, '500')\n",
    "    \n",
    "    summary = test_summaries[key]['summary']\n",
    "    #split into words\n",
    "    summary = summary.split()\n",
    "\n",
    "    summary = summary[:75]\n",
    "    summary = ' '.join(summary)\n",
    "    #split into sentences\n",
    "    sentences = split_into_sentences(summary)\n",
    "    if sentences == []:\n",
    "        sentences = summary\n",
    "    else:\n",
    "        sentences = sentences[0]\n",
    "\n",
    "    #check if sentences is of type list\n",
    "    if type(sentences) == list:\n",
    "        sentences = sentences[0]\n",
    "    \n",
    "\n",
    "    #strip sentence of all non alphanumeric characters\n",
    "    sentences = re.sub(r'[^\\w\\s]', '', sentences)\n",
    "    print('summ: ', sentences)\n",
    "    save_path = f'./fake/summary/ft/sf{key}'\n",
    "\n",
    "    gen_image(save_path, sentences, finetuned, '500')\n",
    "    save_path = f'./fake/summary/base/sb{key}'\n",
    "    gen_image(save_path, sentences, base, 'ddim500')\n",
    "\n",
    "\n",
    "    print('\\n')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FLAGS=\"--image_size 64 --num_channels 128 --num_res_blocks 3 --learn_sigma True\"\n",
    "DIFFUSION_FLAGS=\"--diffusion_steps 4000 --noise_schedule cosine\"\n",
    "TRAIN_FLAGS=\"--lr 1e-4 --batch_size 32\"\n",
    "%env OPENAI_LOGDIR = ./final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train \n",
    "#SET DATA DIR TO FOLDER OF ALL OF YOUR DATA WHICH IS *.JPG FILES\n",
    "!mpiexec -n 1 python guided-diffusion/scripts/image_train.py --resume_checkpoint ./final_model/final/final.pt --data_dir train/data --image_size 64 --num_channels 128 --num_res_blocks 3 --learn_sigma True --lr 1e-4 --batch_size 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train MODEL_FLAGS=\"--image_size 64 --num_channels 128 --num_res_blocks 3 --learn_sigma True\"\n",
    "DIFFUSION_FLAGS=\"--diffusion_steps 4000 --noise_schedule cosine\"\n",
    "TRAIN_FLAGS=\"--lr 1e-4 --batch_size 32\"\n",
    "%env OPENAI_LOGDIR = ./ckpt_model2\n",
    "\n",
    "#SET DATA DIR TO FOLDER OF ALL OF YOUR DATA WHICH IS *.JPG FILES\n",
    "!mpiexec -n 1 python guided-diffusion/scripts/image_train.py --resume_checkpoint ./ckpt_model/final0.pt --data_dir data2 $MODEL_FLAGS $TRAIN_FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAMPLE FINE-TUNED MODEL \n",
    "#CHANGE MODEL PATH TO TRAINED MODEL PATH (PROLLY IN CKPT_MODEL)\n",
    "SAMPLE_FLAGS=\"--batch_size 4 --num_samples 1024 --timestep_respacing 250\"\n",
    "!python guided-diffusion/scripts/image_sample.py --model_path ./ckpt_model/final0.pt $SAMPLE_FLAGS $MODEL_FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GET BASE MODEL AND CLASSIFER\n",
    "!curl -OL \"https://openaipublic.blob.core.windows.net/diffusion/jul-2021/64x64_classifier.pt\"\n",
    "!curl -OL \"https://openaipublic.blob.core.windows.net/diffusion/jul-2021/64x64_diffusion.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#SAMPLE BASE MODEL\n",
    "SAMPLE_FLAGS=\"--batch_size 4 --num_samples 128 --timestep_respacing 250\"\n",
    "\n",
    "\n",
    "MODEL_FLAGS=\"--attention_resolutions 32,16,8 --class_cond True --diffusion_steps 1000 --dropout 0.1 --image_size 64 --learn_sigma True --noise_schedule cosine --num_channels 192 --num_head_channels 64 --num_res_blocks 3 --resblock_updown True --use_new_attention_order True --use_fp16 True --use_scale_shift_norm True\"\n",
    "!python guided-diffusion/scripts/classifier_sample.py $MODEL_FLAGS --classifier_scale 0.0 --classifier_path ./64x64_classifier.pt --classifier_depth 4 --model_path ./64x64_diffusion.pt $SAMPLE_FLAGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP GUIDED DIFFUSION (WORK IN PROGRESS... )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YHOj78Yvx8jP"
   },
   "outputs": [],
   "source": [
    "# Define necessary functions\n",
    "\n",
    "def fetch(url_or_path):\n",
    "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
    "        r = requests.get(url_or_path)\n",
    "        r.raise_for_status()\n",
    "        fd = io.BytesIO()\n",
    "        fd.write(r.content)\n",
    "        fd.seek(0)\n",
    "        return fd\n",
    "    return open(url_or_path, 'rb')\n",
    "\n",
    "\n",
    "def parse_prompt(prompt):\n",
    "    if prompt.startswith('http://') or prompt.startswith('https://'):\n",
    "        vals = prompt.rsplit(':', 2)\n",
    "        vals = [vals[0] + ':' + vals[1], *vals[2:]]\n",
    "    else:\n",
    "        vals = prompt.rsplit(':', 1)\n",
    "    vals = vals + ['', '1'][len(vals):]\n",
    "    return vals[0], float(vals[1])\n",
    "\n",
    "\n",
    "class MakeCutouts(nn.Module):\n",
    "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
    "        super().__init__()\n",
    "        self.cut_size = cut_size\n",
    "        self.cutn = cutn\n",
    "        self.cut_pow = cut_pow\n",
    "\n",
    "    def forward(self, input):\n",
    "        sideY, sideX = input.shape[2:4]\n",
    "        max_size = min(sideX, sideY)\n",
    "        min_size = min(sideX, sideY, self.cut_size)\n",
    "        cutouts = []\n",
    "        for _ in range(self.cutn):\n",
    "            size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
    "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
    "            offsety = torch.randint(0, sideY - size + 1, ())\n",
    "            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
    "            cutouts.append(F.adaptive_avg_pool2d(cutout, self.cut_size))\n",
    "        return torch.cat(cutouts)\n",
    "\n",
    "\n",
    "def spherical_dist_loss(x, y):\n",
    "    x = F.normalize(x, dim=-1)\n",
    "    y = F.normalize(y, dim=-1)\n",
    "    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)\n",
    "\n",
    "\n",
    "def tv_loss(input):\n",
    "    \"\"\"L2 total variation loss, as in Mahendran et al.\"\"\"\n",
    "    input = F.pad(input, (0, 1, 0, 1), 'replicate')\n",
    "    x_diff = input[..., :-1, 1:] - input[..., :-1, :-1]\n",
    "    y_diff = input[..., 1:, :-1] - input[..., :-1, :-1]\n",
    "    return (x_diff**2 + y_diff**2).mean([1, 2, 3])\n",
    "\n",
    "\n",
    "def range_loss(input):\n",
    "    return (input - input.clamp(-1, 1)).pow(2).mean([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fpbody2NCR7w"
   },
   "outputs": [],
   "source": [
    "# Model settings\n",
    "\n",
    "model_config = model_and_diffusion_defaults()\n",
    "model_config.update({\n",
    "    #'attention_resolutions': '32, 16, 8',\n",
    "    'class_cond': False,\n",
    "    'diffusion_steps': 4000,\n",
    "    'rescale_timesteps': True,\n",
    "    'timestep_respacing': '250',  # Modify this value to decrease the number of\n",
    "                                   # timesteps.\n",
    "    'image_size': 64,\n",
    "    'learn_sigma': True,\n",
    "    'noise_schedule': 'linear',\n",
    "    'num_channels': 128,\n",
    "    #'num_head_channels': 64,\n",
    "    'num_res_blocks': 3,\n",
    "    #'resblock_updown': True,\n",
    "    'use_checkpoint': False,\n",
    "    'use_fp16': True,\n",
    "    'use_scale_shift_norm': True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VnQjGugaDZPJ"
   },
   "outputs": [],
   "source": [
    "# Load models\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "model, diffusion = create_model_and_diffusion(**model_config)\n",
    "model.load_state_dict(torch.load('final_model/final/final.pt', map_location='cpu'))\n",
    "model.requires_grad_(False).eval().to(device)\n",
    "if model_config['use_fp16']:\n",
    "    model.convert_to_fp16()\n",
    "\n",
    "clip_model = clip.load('ViT-B/16', jit=False)[0].eval().requires_grad_(False).to(device)\n",
    "clip_size = clip_model.visual.input_resolution\n",
    "normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                                 std=[0.26862954, 0.26130258, 0.27577711])\n",
    "lpips_model = lpips.LPIPS(net='vgg').to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9zY-8I90LkC6"
   },
   "source": [
    "## Settings for this run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U0PwzFZbLfcy"
   },
   "outputs": [],
   "source": [
    "prompts = ['CIA black site detainee served as training prop to teach interrogators torture techniques']\n",
    "image_prompts = []\n",
    "batch_size = 16\n",
    "clip_guidance_scale = 1000  # Controls how much the image should look like the prompt.\n",
    "tv_scale = 150              # Controls the smoothness of the final output.\n",
    "range_scale = 50            # Controls how far out of range RGB values are allowed to be.\n",
    "cutn = 4\n",
    "n_batches = 1\n",
    "init_image = None   # This can be an URL or Colab local path and must be in quotes.\n",
    "skip_timesteps = 0  # This needs to be between approx. 200 and 500 when using an init image.\n",
    "                    # Higher values make the output look more like the init.\n",
    "init_scale = 0      # This enhances the effect of the init image, a good value is 1000.\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nf9hTc8YLoLx"
   },
   "source": [
    "### Actually do the run..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X5gODNAMEUCR"
   },
   "outputs": [],
   "source": [
    "def do_run():\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    make_cutouts = MakeCutouts(clip_size, cutn)\n",
    "    side_x = side_y = model_config['image_size']\n",
    "\n",
    "    target_embeds, weights = [], []\n",
    "\n",
    "    for prompt in prompts:\n",
    "        txt, weight = parse_prompt(prompt)\n",
    "        target_embeds.append(clip_model.encode_text(clip.tokenize(txt).to(device)).float())\n",
    "        weights.append(weight)\n",
    "\n",
    "    for prompt in image_prompts:\n",
    "        path, weight = parse_prompt(prompt)\n",
    "        img = Image.open(fetch(path)).convert('RGB')\n",
    "        img = TF.resize(img, min(side_x, side_y, *img.size), transforms.InterpolationMode.LANCZOS)\n",
    "        batch = make_cutouts(TF.to_tensor(img).unsqueeze(0).to(device))\n",
    "        embed = clip_model.encode_image(normalize(batch)).float()\n",
    "        target_embeds.append(embed)\n",
    "        weights.extend([weight / cutn] * cutn)\n",
    "\n",
    "    target_embeds = torch.cat(target_embeds)\n",
    "    weights = torch.tensor(weights, device=device)\n",
    "    if weights.sum().abs() < 1e-3:\n",
    "        raise RuntimeError('The weights must not sum to 0.')\n",
    "    weights /= weights.sum().abs()\n",
    "\n",
    "    init = None\n",
    "    if init_image is not None:\n",
    "        init = Image.open(fetch(init_image)).convert('RGB')\n",
    "        init = init.resize((side_x, side_y), Image.LANCZOS)\n",
    "        init = TF.to_tensor(init).to(device).unsqueeze(0).mul(2).sub(1)\n",
    "\n",
    "    cur_t = None\n",
    "\n",
    "    def cond_fn(x, t, out, y=None):\n",
    "        n = x.shape[0]\n",
    "        fac = diffusion.sqrt_one_minus_alphas_cumprod[cur_t]\n",
    "        x_in = out['pred_xstart'] * fac + x * (1 - fac)\n",
    "        clip_in = normalize(make_cutouts(x_in.add(1).div(2)))\n",
    "        image_embeds = clip_model.encode_image(clip_in).float()\n",
    "        dists = spherical_dist_loss(image_embeds.unsqueeze(1), target_embeds.unsqueeze(0))\n",
    "        dists = dists.view([cutn, n, -1])\n",
    "        losses = dists.mul(weights).sum(2).mean(0)\n",
    "        tv_losses = tv_loss(x_in)\n",
    "        range_losses = range_loss(out['pred_xstart'])\n",
    "        loss = losses.sum() * clip_guidance_scale + tv_losses.sum() * tv_scale + range_losses.sum() * range_scale\n",
    "        if init is not None and init_scale:\n",
    "            init_losses = lpips_model(x_in, init)\n",
    "            loss = loss + init_losses.sum() * init_scale\n",
    "        return -torch.autograd.grad(loss, x)[0]\n",
    "\n",
    "    if model_config['timestep_respacing'].startswith('ddim'):\n",
    "        sample_fn = diffusion.ddim_sample_loop_progressive\n",
    "    else:\n",
    "        sample_fn = diffusion.p_sample_loop_progressive\n",
    "\n",
    "    for i in range(n_batches):\n",
    "        cur_t = diffusion.num_timesteps - skip_timesteps - 1\n",
    "\n",
    "        samples = sample_fn(\n",
    "            model,\n",
    "            (batch_size, 3, side_y, side_x),\n",
    "            clip_denoised=False,\n",
    "            model_kwargs={},\n",
    "            cond_fn=cond_fn,\n",
    "            progress=True,\n",
    "            skip_timesteps=skip_timesteps,\n",
    "            init_image=init,\n",
    "            randomize_class=True,\n",
    "            cond_fn_with_grad=True,\n",
    "        )\n",
    "\n",
    "        for j, sample in enumerate(samples):\n",
    "            cur_t -= 1\n",
    "            if j % 100 == 0 or cur_t == -1:\n",
    "                print()\n",
    "                for k, image in enumerate(sample['pred_xstart']):\n",
    "                    filename = f'progress_{i * batch_size + k:05}.png'\n",
    "                    TF.to_pil_image(image.add(1).div(2).clamp(0, 1)).save(filename)\n",
    "                    tqdm.write(f'Batch {i}, step {j}, output {k}:')\n",
    "                    display.display(display.Image(filename))\n",
    "\n",
    "gc.collect()\n",
    "do_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zip every file in fake folder recursively\n",
    "!zip -r fake.zip fake/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CLIP Guided Diffusion HQ 256x256.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "interpreter": {
   "hash": "cf6277a89a1511f2ce434acab1b00d04286bdb855dc5571e1cf1eeb6f4b6a298"
  },
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p38)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
